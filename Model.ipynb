{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMPzVlvjJnTfkf/Pefv6g9y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"ra-hhfP8lgkM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHHSbmw7azT6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762189686067,"user_tz":-330,"elapsed":7861,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"70795675-1875-490a-bf37-8756475279c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive mounted.\n","\n","Using device: cuda\n","Facial Model will be saved to: /content/drive/MyDrive/best_fer2013_cnn.pth\n"]}],"source":["# --- 1. INSTALLS AND IMPORTS ---\n","!pip install -q scikit-learn pandas numpy Pillow\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import os\n","from google.colab import drive\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# --- 2. DRIVE MOUNT ---\n","print(\"Mounting Google Drive...\")\n","if 'google.colab' in sys.modules:\n","    drive.mount('/content/drive')\n","print(\"Drive mounted.\")\n","\n","# --- 3. CONFIGURATION (FACIAL MODEL) ---\n","# >>>>>> CRITICAL STEP: REPLACE THIS PATH with the EXACT path to your fer2013.csv file on Drive <<<<<<\n","CSV_FILE_PATH = '/content/drive/MyDrive/multimodel-fusion/fer2013.csv'\n","MODEL_SAVE_PATH_FACE = '/content/drive/MyDrive/best_fer2013_cnn.pth'\n","\n","NUM_CLASSES = 7  # Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n","BATCH_SIZE = 64\n","EPOCHS = 20\n","LEARNING_RATE = 0.001\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","EMOTION_MAP_FACE = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n","\n","print(f\"\\nUsing device: {DEVICE}\")\n","print(f\"Facial Model will be saved to: {MODEL_SAVE_PATH_FACE}\")"]},{"cell_type":"code","source":["# --- 4. CUSTOM PYTORCH DATASET CLASS ---\n","class FER2013Dataset(Dataset):\n","    def __init__(self, data_frame, transform=None):\n","        self.data_frame = data_frame\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data_frame)\n","\n","    def __getitem__(self, idx):\n","        # 1. Extract pixel string and convert to 48x48 numpy array\n","        pixel_string = self.data_frame.iloc[idx]['pixels']\n","        pixels = np.fromstring(pixel_string, dtype=int, sep=' ').reshape(48, 48)\n","        image = pixels.astype(np.uint8)\n","\n","        # 2. Convert to PIL Image\n","        image = Image.fromarray(image).convert('L') # 'L' for grayscale\n","\n","        # 3. Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # 4. Get label\n","        label = self.data_frame.iloc[idx]['emotion']\n","        return image, torch.tensor(label, dtype=torch.long)\n","\n","# --- 5. DATA LOADING AND PREPROCESSING ---\n","try:\n","    df = pd.read_csv(CSV_FILE_PATH)\n","    print(\"✅ Facial DataFrame loaded successfully from Drive.\")\n","except FileNotFoundError:\n","    print(f\"❌ FATAL ERROR: CSV file not found at {CSV_FILE_PATH}. Please check your path and Cell 1.\")\n","    sys.exit(1)\n","\n","# Standard FER-2013 Split based on the 'Usage' column\n","df_train = df[df['Usage'] == 'Training']\n","df_val = df[df['Usage'] == 'PrivateTest']\n","df_test = df[df['Usage'] == 'PublicTest']\n","\n","print(f\"Unique 'Usage' values found in CSV: {df['Usage'].unique()}\")\n","\n","# Data Transformations (Augmentation is used on training set)\n","train_transform = transforms.Compose([\n","    transforms.RandomCrop(48, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# Create Dataset and DataLoader instances\n","train_dataset = FER2013Dataset(df_train, transform=train_transform)\n","val_dataset = FER2013Dataset(df_val, transform=test_transform)\n","test_dataset = FER2013Dataset(df_test, transform=test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Lb75aVgbwTz","executionInfo":{"status":"ok","timestamp":1762189705947,"user_tz":-330,"elapsed":11526,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"6a13f6fd-fd55-47d4-8509-2db43de82277"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Facial DataFrame loaded successfully from Drive.\n","Unique 'Usage' values found in CSV: ['Training' 'PublicTest' 'PrivateTest']\n","Train samples: 28709, Validation samples: 3589, Test samples: 3589\n"]}]},{"cell_type":"code","source":["# --- 6. CNN MODEL ARCHITECTURE (Facial) ---\n","class EmotionCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(EmotionCNN, self).__init__()\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(0.3),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128), nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(0.3)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(128 * 6 * 6, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","model_face = EmotionCNN(NUM_CLASSES).to(DEVICE)\n","print(\"Facial Model architecture defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQy0z5LMb0dT","executionInfo":{"status":"ok","timestamp":1762189709873,"user_tz":-330,"elapsed":245,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"4fd42051-979e-40a0-9898-5336f86e5944"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Facial Model architecture defined.\n"]}]},{"cell_type":"code","source":["# --- 7. TRAINING AND TESTING FUNCTIONS (Facial) ---\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_face.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n","\n","def train_model(model, loader, criterion, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    for images, labels in loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * images.size(0)\n","    return total_loss / len(loader.dataset)\n","\n","def evaluate_model(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return correct / total\n","\n","# --- 8. MAIN TRAINING LOOP (Facial) ---\n","best_val_accuracy = 0.0\n","print(\"\\nStarting Facial Emotion Model Training...\")\n","for epoch in range(EPOCHS):\n","    train_loss = train_model(model_face, train_loader, criterion, optimizer, DEVICE)\n","    val_accuracy = evaluate_model(model_face, val_loader, DEVICE)\n","\n","    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy*100:.2f}%')\n","\n","    # Save the best model to Drive\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save(model_face.state_dict(), MODEL_SAVE_PATH_FACE)\n","        print(f\">>> Best Facial model saved to Drive: {MODEL_SAVE_PATH_FACE}\")\n","\n","# --- 9. FINAL TEST EVALUATION ---\n","model_face.load_state_dict(torch.load(MODEL_SAVE_PATH_FACE, map_location=DEVICE))\n","final_test_accuracy = evaluate_model(model_face, test_loader, DEVICE)\n","print(f\"\\nFINAL TEST ACCURACY of BEST FACIAL MODEL: {final_test_accuracy*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSjKxD7xb3A9","outputId":"2ed05101-78db-4d56-ff78-4221c16455c8","executionInfo":{"status":"ok","timestamp":1762190114283,"user_tz":-330,"elapsed":401545,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Facial Emotion Model Training...\n","Epoch [1/20], Train Loss: 1.7609, Val Accuracy: 41.29%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [2/20], Train Loss: 1.5500, Val Accuracy: 47.06%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [3/20], Train Loss: 1.4559, Val Accuracy: 51.16%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [4/20], Train Loss: 1.4079, Val Accuracy: 52.55%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [5/20], Train Loss: 1.3647, Val Accuracy: 54.97%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [6/20], Train Loss: 1.3379, Val Accuracy: 55.08%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [7/20], Train Loss: 1.3140, Val Accuracy: 55.75%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [8/20], Train Loss: 1.2998, Val Accuracy: 57.34%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [9/20], Train Loss: 1.2804, Val Accuracy: 56.62%\n","Epoch [10/20], Train Loss: 1.2696, Val Accuracy: 55.28%\n","Epoch [11/20], Train Loss: 1.2604, Val Accuracy: 56.95%\n","Epoch [12/20], Train Loss: 1.2471, Val Accuracy: 57.68%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [13/20], Train Loss: 1.2361, Val Accuracy: 58.85%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [14/20], Train Loss: 1.2257, Val Accuracy: 57.82%\n","Epoch [15/20], Train Loss: 1.2185, Val Accuracy: 59.46%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [16/20], Train Loss: 1.2049, Val Accuracy: 59.13%\n","Epoch [17/20], Train Loss: 1.1973, Val Accuracy: 59.60%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","Epoch [18/20], Train Loss: 1.1925, Val Accuracy: 58.65%\n","Epoch [19/20], Train Loss: 1.1863, Val Accuracy: 59.21%\n","Epoch [20/20], Train Loss: 1.1798, Val Accuracy: 60.41%\n",">>> Best Facial model saved to Drive: /content/drive/MyDrive/best_fer2013_cnn.pth\n","\n","FINAL TEST ACCURACY of BEST FACIAL MODEL: 58.93%\n"]}]},{"cell_type":"code","source":["# --- 10. INSTALLS AND IMPORTS (Speech) ---\n","!pip install -q librosa soundfile\n","\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import librosa\n","import soundfile\n","import glob\n","import sys\n","\n","# --- 11. CONFIGURATION (SPEECH MODEL) ---\n","# >>>>>> CRITICAL STEP: REPLACE THIS PATH with the EXACT path to the folder that contains your RAVDESS Actor_xx folders on your Drive <<<<<<\n","RAVDESS_PATH = '/content/drive/MyDrive/multimodel-fusion/archive (2)'\n","MODEL_SAVE_PATH_SPEECH = '/content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth'\n","\n","# Fixed parameters for audio processing\n","SAMPLE_RATE = 22050\n","N_MFCC = 40\n","MAX_PAD_LENGTH = 174\n","BATCH_SIZE = 64\n","\n","# Emotion Mapping based on RAVDESS filename (Part 3)\n","EMOTION_MAP_RAVDESS = {\n","    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n","    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n","}\n","NUM_CLASSES_SPEECH = len(EMOTION_MAP_RAVDESS)\n","print(f\"Speech model will be saved to: {MODEL_SAVE_PATH_SPEECH}\")\n","\n","\n","# --- 12. FEATURE EXTRACTION FUNCTION (MFCCs) ---\n","def extract_feature(file_name, **kwargs):\n","    mfcc = kwargs.get(\"mfcc\")\n","    try:\n","        with soundfile.SoundFile(file_name) as sound_file:\n","            X = sound_file.read(dtype=\"float32\")\n","            sample_rate = sound_file.samplerate\n","\n","            if sample_rate != SAMPLE_RATE:\n","                X = librosa.resample(X, orig_sr=sample_rate, target_sr=SAMPLE_RATE)\n","                sample_rate = SAMPLE_RATE\n","\n","            if mfcc:\n","                result = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=N_MFCC)\n","\n","                # Padding/Truncation\n","                if result.shape[1] > MAX_PAD_LENGTH:\n","                    result = result[:, :MAX_PAD_LENGTH]\n","                elif result.shape[1] < MAX_PAD_LENGTH:\n","                    pad_width = MAX_PAD_LENGTH - result.shape[1]\n","                    result = np.pad(result, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","\n","                return result\n","    except Exception as e:\n","        # Pass silently if file is corrupted, or raise error if needed\n","        return None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_Wb-G_mnjP7","executionInfo":{"status":"ok","timestamp":1762190161435,"user_tz":-330,"elapsed":5185,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"b9e524c2-c181-4414-c2a9-acaa36670586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Speech model will be saved to: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n"]}]},{"cell_type":"code","source":["# --- 13. LOAD DATA AND EXTRACT FEATURES ---\n","def load_data(ravdess_dir):\n","    features = []\n","    labels = []\n","\n","    if not os.path.isdir(ravdess_dir):\n","        print(f\"❌ FATAL ERROR: RAVDESS directory not found at {ravdess_dir}.\")\n","        sys.exit(1)\n","\n","    print(\"Loading RAVDESS data and extracting MFCC features...\")\n","    # Search for all WAV files recursively under the RAVDESS_PATH\n","    search_path = os.path.join(ravdess_dir, '**', '*.wav')\n","\n","    file_list = glob.glob(search_path, recursive=True)\n","    if not file_list:\n","        print(f\"❌ FATAL ERROR: No .wav files found in {ravdess_dir}. Is the folder structure correct?\")\n","        sys.exit(1)\n","\n","    for file in file_list:\n","        basename = os.path.basename(file)\n","        emotion_code = basename.split('-')[2]\n","\n","        if emotion_code in EMOTION_MAP_RAVDESS:\n","            label = EMOTION_MAP_RAVDESS[emotion_code]\n","            feature = extract_feature(file, mfcc=True)\n","\n","            if feature is not None:\n","                features.append(feature.T)\n","                labels.append(label)\n","\n","    return np.array(features), np.array(labels)\n","\n","# Load data (This can take time)\n","X, y = load_data(RAVDESS_PATH)\n","if X.size == 0:\n","    print(\"❌ FATAL ERROR: No audio files loaded.\")\n","    sys.exit(1)\n","print(f\"Total extracted samples: {X.shape[0]}, Feature shape (TimeSteps, N_MFCC): {X.shape[1:]}\")\n","\n","# --- 14. DATA PREPARATION ---\n","encoder = OneHotEncoder()\n","y_encoded = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n","\n","X_train_raw, X_test_raw, y_train_enc, y_test_enc = train_test_split(\n","    X, y_encoded, test_size=0.25, random_state=42, shuffle=True\n",")\n","\n","scaler = StandardScaler()\n","X_train_flat = X_train_raw.reshape(-1, N_MFCC)\n","scaler.fit(X_train_flat)\n","\n","X_train_scaled = scaler.transform(X_train_flat).reshape(X_train_raw.shape)\n","X_test_scaled = scaler.transform(X_test_raw.reshape(-1, N_MFCC)).reshape(X_test_raw.shape)\n","\n","X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train_enc, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test_enc, dtype=torch.float32)\n","\n","train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","test_data = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader_speech = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","test_loader_speech = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","print(f\"Train samples (Speech): {len(train_data)}, Test samples (Speech): {len(test_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJyxZimNnmt0","executionInfo":{"status":"ok","timestamp":1762192803625,"user_tz":-330,"elapsed":2618356,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"dfe0bc65-1051-4f24-a712-c56700603285"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading RAVDESS data and extracting MFCC features...\n","Total extracted samples: 2870, Feature shape (TimeSteps, N_MFCC): (174, 40)\n","Train samples (Speech): 2152, Test samples (Speech): 718\n"]}]},{"cell_type":"code","source":["# --- 15. CNN-LSTM MODEL ARCHITECTURE for SER ---\n","class SpeechEmotionCNN_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(SpeechEmotionCNN_LSTM, self).__init__()\n","\n","        # 1D CNN Layers\n","        self.conv1d = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.2),\n","            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm1d(64), nn.MaxPool1d(kernel_size=2)\n","        )\n","\n","        # LSTM Layer\n","        self.lstm = nn.LSTM(\n","            input_size=64,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        # Fully Connected Classifier\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        # Transpose for Conv1D: [B, N_MFCC, TimeSteps]\n","        x = x.transpose(1, 2)\n","        cnn_out = self.conv1d(x)\n","\n","        # Transpose back for LSTM: [B, seq_len, features]\n","        lstm_input = cnn_out.transpose(1, 2)\n","        lstm_out, (h_n, c_n) = self.lstm(lstm_input)\n","\n","        # Concatenate the final forward and backward hidden states\n","        final_state = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(final_state)\n","        out = self.fc(out)\n","        return out\n","\n","# Model Hyperparameters\n","HIDDEN_SIZE = 128\n","NUM_LAYERS = 2\n","\n","model_speech = SpeechEmotionCNN_LSTM(\n","    input_size=N_MFCC,\n","    hidden_size=HIDDEN_SIZE,\n","    num_layers=NUM_LAYERS,\n","    num_classes=NUM_CLASSES_SPEECH\n",").to(DEVICE)\n","\n","print(\"Speech Model architecture defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOT_6_LgnrM6","executionInfo":{"status":"ok","timestamp":1762192855853,"user_tz":-330,"elapsed":133,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"6d0f03e9-aaa0-4130-a1a2-1581d3dc622b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Speech Model architecture defined.\n"]}]},{"cell_type":"code","source":["# --- 16. TRAINING AND TESTING FUNCTIONS (Speech) ---\n","criterion_speech = nn.CrossEntropyLoss()\n","optimizer_speech = optim.Adam(model_speech.parameters(), lr=LEARNING_RATE)\n","EPOCHS_SPEECH = 50\n","\n","def train_model_speech(model, loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    for X_batch, y_batch in loader:\n","        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","        labels_index = torch.argmax(y_batch, dim=1)\n","\n","        outputs = model(X_batch)\n","        loss = criterion(outputs, labels_index)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * X_batch.size(0)\n","\n","    return running_loss / len(loader.dataset)\n","\n","def evaluate_model_speech(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for X_batch, y_batch in loader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            labels_index = torch.argmax(y_batch, dim=1)\n","\n","            outputs = model(X_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels_index.size(0)\n","            correct += (predicted == labels_index).sum().item()\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","# --- 17. MAIN TRAINING LOOP (Speech) ---\n","best_val_accuracy_speech = 0.0\n","print(\"\\nStarting Speech Model Training...\")\n","for epoch in range(EPOCHS_SPEECH):\n","    train_loss = train_model_speech(model_speech, train_loader_speech, criterion_speech, optimizer_speech, DEVICE)\n","    val_accuracy = evaluate_model_speech(model_speech, test_loader_speech, DEVICE)\n","\n","    print(f'Epoch [{epoch+1}/{EPOCHS_SPEECH}], Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy*100:.2f}%')\n","\n","    # Save the best model to Drive\n","    if val_accuracy > best_val_accuracy_speech:\n","        best_val_accuracy_speech = val_accuracy\n","        torch.save(model_speech.state_dict(), MODEL_SAVE_PATH_SPEECH)\n","        print(f\">>> Best Speech Model saved to Drive: {MODEL_SAVE_PATH_SPEECH}\")\n","\n","# --- 18. FINAL TEST EVALUATION ---\n","model_speech.load_state_dict(torch.load(MODEL_SAVE_PATH_SPEECH, map_location=DEVICE))\n","final_test_accuracy_speech = evaluate_model_speech(model_speech, test_loader_speech, DEVICE)\n","print(f\"\\nFINAL TEST ACCURACY of BEST SPEECH MODEL: {final_test_accuracy_speech*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sq9p3GaDnsFj","executionInfo":{"status":"ok","timestamp":1762192899080,"user_tz":-330,"elapsed":39859,"user":{"displayName":"Vanshika Goyal","userId":"11908295447741305576"}},"outputId":"1ea334b2-87f4-499f-bc8a-96c48f1c8dab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Speech Model Training...\n","Epoch [1/50], Train Loss: 1.8500, Val Accuracy: 36.77%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [2/50], Train Loss: 1.6508, Val Accuracy: 36.35%\n","Epoch [3/50], Train Loss: 1.5590, Val Accuracy: 40.95%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [4/50], Train Loss: 1.5323, Val Accuracy: 39.00%\n","Epoch [5/50], Train Loss: 1.4838, Val Accuracy: 44.15%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [6/50], Train Loss: 1.4744, Val Accuracy: 42.76%\n","Epoch [7/50], Train Loss: 1.3636, Val Accuracy: 46.24%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [8/50], Train Loss: 1.2738, Val Accuracy: 50.56%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [9/50], Train Loss: 1.2181, Val Accuracy: 50.84%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [10/50], Train Loss: 1.1566, Val Accuracy: 50.28%\n","Epoch [11/50], Train Loss: 1.0464, Val Accuracy: 55.29%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [12/50], Train Loss: 0.9834, Val Accuracy: 56.69%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [13/50], Train Loss: 0.8993, Val Accuracy: 64.07%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [14/50], Train Loss: 0.8148, Val Accuracy: 66.30%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [15/50], Train Loss: 0.7410, Val Accuracy: 61.14%\n","Epoch [16/50], Train Loss: 0.7439, Val Accuracy: 67.69%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [17/50], Train Loss: 0.6462, Val Accuracy: 69.36%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [18/50], Train Loss: 0.5908, Val Accuracy: 69.50%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [19/50], Train Loss: 0.5042, Val Accuracy: 70.47%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [20/50], Train Loss: 0.5843, Val Accuracy: 73.40%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [21/50], Train Loss: 0.4818, Val Accuracy: 77.44%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [22/50], Train Loss: 0.3500, Val Accuracy: 77.86%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [23/50], Train Loss: 0.3557, Val Accuracy: 76.60%\n","Epoch [24/50], Train Loss: 0.3186, Val Accuracy: 78.69%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [25/50], Train Loss: 0.2684, Val Accuracy: 81.34%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [26/50], Train Loss: 0.2831, Val Accuracy: 80.92%\n","Epoch [27/50], Train Loss: 0.2627, Val Accuracy: 81.62%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [28/50], Train Loss: 0.2312, Val Accuracy: 82.73%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [29/50], Train Loss: 0.2163, Val Accuracy: 80.36%\n","Epoch [30/50], Train Loss: 0.2071, Val Accuracy: 85.79%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [31/50], Train Loss: 0.2170, Val Accuracy: 85.65%\n","Epoch [32/50], Train Loss: 0.1684, Val Accuracy: 87.88%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [33/50], Train Loss: 0.1256, Val Accuracy: 87.19%\n","Epoch [34/50], Train Loss: 0.1180, Val Accuracy: 88.58%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [35/50], Train Loss: 0.1034, Val Accuracy: 87.88%\n","Epoch [36/50], Train Loss: 0.1114, Val Accuracy: 87.33%\n","Epoch [37/50], Train Loss: 0.0919, Val Accuracy: 86.91%\n","Epoch [38/50], Train Loss: 0.1174, Val Accuracy: 84.54%\n","Epoch [39/50], Train Loss: 0.1583, Val Accuracy: 88.58%\n","Epoch [40/50], Train Loss: 0.1302, Val Accuracy: 87.74%\n","Epoch [41/50], Train Loss: 0.0870, Val Accuracy: 88.16%\n","Epoch [42/50], Train Loss: 0.0671, Val Accuracy: 88.72%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [43/50], Train Loss: 0.0522, Val Accuracy: 90.25%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [44/50], Train Loss: 0.0324, Val Accuracy: 89.28%\n","Epoch [45/50], Train Loss: 0.0399, Val Accuracy: 89.97%\n","Epoch [46/50], Train Loss: 0.0884, Val Accuracy: 89.83%\n","Epoch [47/50], Train Loss: 0.0575, Val Accuracy: 89.83%\n","Epoch [48/50], Train Loss: 0.0516, Val Accuracy: 89.83%\n","Epoch [49/50], Train Loss: 0.0520, Val Accuracy: 90.81%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","Epoch [50/50], Train Loss: 0.0470, Val Accuracy: 91.23%\n",">>> Best Speech Model saved to Drive: /content/drive/MyDrive/best_ravdess_speech_cnnlstm.pth\n","\n","FINAL TEST ACCURACY of BEST SPEECH MODEL: 91.23%\n"]}]}]}